{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"corpus/\"\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Headers and HTML Tags, Tokenization, Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_words = {}\n",
    "i = 0 #for docs_words\n",
    "\n",
    "for f in files:\n",
    "    doc = open(f, errors='ignore')\n",
    "    html = doc.read()\n",
    "    \n",
    "    index = html.find(\"<html\")\n",
    "    if index == -1 :\n",
    "        html.find(\"<Html\")\n",
    "    elif index == -1 : \n",
    "        html.find(\"<HTML\")\n",
    "    html = html[index:]\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    #print(\"Doc : \" + text + \"\\n\\n\")\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    words_pos = {}\n",
    "    j = 0; #for words_pos\n",
    "    for t in tokens:\n",
    "        t = t.lower() #lowering Case\n",
    "        t = t.replace(\"'\", \"\")\n",
    "        if t not in words_pos.keys():\n",
    "            words_pos[t] = []\n",
    "        if t in words_pos.keys():\n",
    "            words_pos[t].append(j)\n",
    "        j = j + 1\n",
    "    \n",
    "    docs_words[i] = words_pos\n",
    "    i = i + 1\n",
    "\n",
    "#print(docs_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stoplist.txt\", \"r\") \n",
    "stoplist = f.read().splitlines() #Stoplist words\n",
    "\n",
    "for i in range(len(docs_words)):\n",
    "    for key in list(docs_words[i]):\n",
    "        if key in stoplist or len(key) == 1:\n",
    "            docs_words[i].pop(key,None)\n",
    "#print(docs_words)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping up terms and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = {}\n",
    "\n",
    "for i in range(len(docs_words)):\n",
    "    for key in docs_words[i].keys():\n",
    "        docs_pos = {}\n",
    "        docs_pos[i] = docs_words[i].get(key)\n",
    "        if key not in words_docs.keys():\n",
    "            words_docs[key] = docs_pos\n",
    "        elif key in words_docs.keys():\n",
    "             words_docs[key][i] = docs_pos[i]\n",
    "\n",
    "#print(words_docs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for key in list(words_docs):\n",
    "    stem = ps.stem(key)\n",
    "    if stem != key:\n",
    "        if stem not in words_docs.keys():\n",
    "            words_docs[stem] = words_docs.pop(key)\n",
    "        elif stem in words_docs.keys():\n",
    "            dictn = words_docs.pop(key)\n",
    "            for doc in dictn.keys():\n",
    "                if doc not in words_docs[stem].keys():\n",
    "                    words_docs[stem][doc] = dictn[doc]\n",
    "                elif doc in words_docs[stem].keys():\n",
    "                    words_docs[stem][doc] = words_docs[stem][doc] + dictn[doc]\n",
    "\n",
    "#print(docs_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing docids.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_docs = open(\"docids.txt\", 'w')\n",
    "i = 0\n",
    "for f in files:\n",
    "    doc_name = os.path.basename(f)\n",
    "    f_docs.write(str(i) + \"\\t\" + doc_name + \"\\n\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing termids.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_terms = open(\"termids.txt\", 'w', errors=\"ignore\")\n",
    "id = 0\n",
    "\n",
    "for key in words_docs.keys():\n",
    "    f_terms.write(str(id) + \"\\t\" + key + \"\\n\")\n",
    "    id = id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{25: [4187], 26: [4187], 102: [722], 171: [4081], 234: [1325, 196, 476, 508, 2533], 316: [114, 5366], 322: [7212], 366: [109], 425: [2619], 444: [223], 511: [5003], 529: [612], 547: [2453], 554: [91], 571: [2558], 579: [929], 580: [929], 582: [929], 583: [929], 586: [929], 587: [929], 588: [929], 591: [929], 592: [929], 595: [108, 2037], 612: [929], 614: [929], 616: [929], 678: [15027, 15071], 699: [65], 782: [784], 846: [111, 4521], 1032: [3237], 1046: [3620], 1050: [1068, 702, 1041], 1059: [2294, 2537, 2206], 1060: [76, 273, 503, 689, 1038, 1099, 1391, 685], 1072: [933, 1474], 1288: [51, 188, 204], 1318: [76, 273, 503, 1046, 1106, 1398], 1384: [2630], 1386: [112348], 1391: [3141], 1400: [3282], 1404: [5045, 5187, 5295], 1405: [5545, 5687, 5795], 1409: [3454], 1414: [2735], 1415: [3276], 1417: [2939], 1637: [3835, 3320, 3826], 1653: [107, 4908], 1656: [80, 775, 881, 902, 1120, 1557], 1657: [80, 775, 881, 902, 1120, 1557], 1661: [32], 1700: [1720], 1709: [504, 423], 1794: [98, 226, 417, 426, 456, 547, 552, 557, 601, 130, 258], 1913: [328, 3722, 4910], 1934: [112, 1059, 2527], 1946: [8099, 8113, 8123], 2021: [762], 2118: [1192, 1194, 1201], 2166: [520, 524, 529, 535, 17993], 2246: [689, 747], 2289: [403], 2326: [1299], 2329: [1299], 2331: [1184, 1958, 2124], 2332: [1184, 1958, 2124], 2338: [549, 1439], 2347: [658, 1580, 1950, 2116, 3042], 2352: [89, 4477], 2358: [89, 4251], 2454: [3076], 2460: [645], 2495: [7212], 2562: [136], 2585: [793], 2590: [3108], 2724: [8325, 8882, 9165], 2726: [1578, 1748, 2378], 2748: [1415], 2778: [4165], 2780: [1, 337, 462, 560, 729, 737, 1021, 1060, 1189, 541, 632], 2781: [336, 1170, 1298], 2884: [1917, 2067], 2885: [1917, 2067], 2998: [666], 3020: [9351, 10524, 20111], 3145: [131], 3157: [472], 3191: [99, 5729], 3192: [93, 5470], 3193: [100, 5739], 3194: [378, 782, 2184], 3227: [2975], 3301: [855], 3322: [8378], 3328: [1117, 1153, 1178, 1143], 3330: [156], 3333: [176, 5850], 3336: [100, 5769], 3337: [112, 3817], 3348: [2162, 2183, 2189, 2254, 2198], 3350: [303], 3352: [156], 3353: [2192, 2213, 2219, 2284, 2228], 3355: [156], 3426: [6911, 6938, 6975, 7062, 7077, 7111, 16804, 16837, 17053, 7026, 16902], 117: [766], 183: [605], 397: [300], 498: [66, 196], 640: [479], 652: [1421], 779: [365, 367], 801: [162], 880: [785], 882: [2624], 1168: [547], 1173: [1436], 1265: [712], 1685: [792], 1713: [390], 1836: [4074], 1859: [3723], 1888: [1419], 1990: [279], 2056: [1131], 2169: [7641, 7643], 2175: [940], 2176: [940], 2177: [940], 2223: [1514], 2225: [481], 2227: [1499], 2344: [296], 2484: [685], 2819: [298], 2887: [365], 2992: [429], 3112: [829], 3265: [1007], 3309: [461], 3312: [2862, 2969], 3423: [19746], 3456: [1325]}\n"
     ]
    }
   ],
   "source": [
    "print(words_docs[\"appl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
