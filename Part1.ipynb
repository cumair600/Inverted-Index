{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs/file1\n",
      "Docs/file2\n",
      "Docs/file3.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"Docs/\"\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))\n",
    "    \n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Headers and HTML Tags, Tokenization, Lower Casing, StopListing, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'dark': [0, 18, 27, 54, 69, 96, 104, 134], \"chocolate'\": [1, 55, 187, 198, 218], 'health': [2, 5, 23, 56, 188, 200, 235], 'benefit': [3, 189], 'chocol': [4, 11, 16, 19, 28, 38, 70, 97, 105, 124, 135, 158, 163, 167, 172, 177, 214, 221, 232, 234], 'basic': [6, 7, 202], \"it'\": [8], 'wish': [9], 'think': [10], 'good': [12, 99, 175], 'studi': [13, 51, 84, 95, 201], 'show': [14, 113], 'eat': [15], 'primarili': [17], 'contribut': [20], 'improv': [21, 110, 194], 'cardiovascular': [22, 87], 'pack': [24], 'natur': [25, 48, 75, 159], 'antioxid': [26, 66, 67, 74, 86, 203], 'cocoa': [29, 42, 62, 71, 98], 'sit': [30], 'good-for-y': [31], 'categori': [32], 'green': [33], 'tea': [34], 'blueberri': [35], \"that'\": [36], 'becaus': [37, 58], 'come': [39], 'cacao': [40, 45, 227], 'bean': [41, 43], 'grow': [44], 'tree': [46], 'full': [47], 'plant': [49], 'nutrient': [50, 166], 'date': [52], 'highlight': [53], 'valu': [57], 'ha': [59, 106, 136, 254, 260], 'highest': [60], 'percentag': [61], 'solid': [63], 'therefor': [64], 'flavanol': [65], 'powerhous': [68, 204], 'rich': [72], 'cell-protect': [73], 'compound': [76], 'found': [77], 'fruit': [78], 'veget': [79], 'grain': [80], 'nut': [81], 'read': [82, 122], 'scientist': [83, 230], 'regard': [85], 'diseas': [88], 'prematur': [89], 'age': [90], 'cancer': [91], 'matter': [92, 205], 'heart': [93, 100, 125, 206], 'recent': [94], 'short-term': [101], 'clinic': [102], 'trial': [103], 'reduc': [107], 'blood': [108, 111, 129, 145, 210], 'pressur': [109], 'flow': [112], 'mild': [114], 'anti-clot': [115], 'effect': [116], 'help': [117], 'prevent': [118], 'plaqu': [119], 'format': [120], 'arteri': [121], 'research': [123, 185], 'sweet': [126, 132, 207], 'news': [127, 208], 'manag': [128, 209], 'sugar': [130, 146, 211], 'despit': [131], 'reput': [133], 'low': [137], 'glycem': [138], 'index': [139, 242], 'similar': [140], 'oatmeal': [141], 'mean': [142], 'doe': [143, 171, 178], 'send': [144, 248, 262], 'spike': [147], 'find': [148, 184], 'surpris': [149], 'chemistri': [150], 'favorit': [151], 'treat': [152], 'vital': [153, 212], 'miner': [154, 162, 213], 'ani': [155], 'plant-bas': [156], 'food': [157], 'contain': [160], 'array': [161], 'stack': [164], 'critic': [165], 'brain': [168, 179, 215], 'tast': [169], 'whi': [170], 'make': [173], 'feel': [174], 'discov': [176], 'emerg': [180, 216], 'scienc': [181, 217], 'buoy': [182], 'posit': [183], 'conduct': [186], 'includ': [190], 'potenti': [191], 'cancer-fight': [192], 'abil': [193], 'cognit': [195], 'function': [196], 'learn': [197], 'futur': [199], 'nutrit': [219], 'breakdown': [220], 'myth': [222], 'fact': [223], 'part': [224], 'balanc': [225], 'diet': [226], 'ancient': [228], 'medicin': [229], 'forefront': [231], 'pet': [233], 'resourc': [236], 'legal': [237], 'info': [238], 'privaci': [239, 246], 'polici': [240, 247], 'site': [241, 244], 'contact': [243], 'sponsor': [245], 'friend': [249], 'requir': [250], 'field': [251], 'thank': [252, 265], 'messag': [253], 'sorri': [255], 'owner': [256], 'email': [257, 264], 'address': [258], 'enter': [259], 'request': [261], 'unsolicit': [263]}, 1: {'aol': [0, 3, 15, 60, 80], 'lifestream': [1, 4, 61], 'login': [2], 'everyth': [5, 10], 'peopl': [6, 11], 'place': [7, 12, 37, 45], 'sign': [8, 13, 20, 38, 56, 59, 63, 64], 'insign': [9], 'aim': [14], 'account': [16, 18, 25, 27], 'exist': [17, 26], 'facebook': [19, 28], 'start': [21], 'connect': [22], 'network': [23, 31, 35, 42, 44], 'creat': [24], 'stream': [29], 'multipl': [30], 'read': [32], 'comment': [33], 'post': [34], 'handi': [36], 'find': [39], 'now.on': [40], 'ani': [41], 'search': [43], 'stay': [46], 'date': [47], \"you'r\": [48], \"there'\": [49], 'app': [50], 'smartphon': [51], \"won't\": [52], 'miss': [53], 'thing': [54], 'learn': [55], 'simplifi': [57], 'social': [58], 'download': [62], 'feedback': [65], 'updat': [66, 69], 'term': [67], 'servic': [68], 'privaci': [70], 'polici': [71, 75], 'trademark': [72], 'bulk': [73], 'email': [74], 'advertis': [76], 'ad': [77], 'aol.com': [78], '2012': [79], 'right': [81], 'reserv': [82]}, 2: {'aol': [0, 3, 15, 60, 80], 'lifestream': [1, 4, 61], 'login': [2], 'everyth': [5, 10], 'peopl': [6, 11], 'place': [7, 12, 37, 45], 'sign': [8, 13, 20, 38, 56, 59, 63, 64], 'insign': [9], 'aim': [14], 'account': [16, 18, 25, 27], 'exist': [17, 26], 'facebook': [19, 28], 'start': [21], 'connect': [22], 'network': [23, 31, 35, 42, 44], 'creat': [24], 'stream': [29], 'multipl': [30], 'read': [32], 'comment': [33], 'post': [34], 'handi': [36], 'find': [39], 'now.on': [40], 'ani': [41], 'search': [43], 'stay': [46], 'date': [47], \"you'r\": [48], \"there'\": [49], 'app': [50], 'smartphon': [51], \"won't\": [52], 'miss': [53], 'thing': [54], 'learn': [55], 'simplifi': [57], 'social': [58], 'download': [62], 'feedback': [65], 'updat': [66, 69], 'term': [67], 'servic': [68], 'privaci': [70], 'polici': [71, 75], 'trademark': [72], 'bulk': [73], 'email': [74], 'advertis': [76], 'ad': [77], 'aol.com': [78], '2012': [79], 'right': [81], 'reserv': [82]}}\n"
     ]
    }
   ],
   "source": [
    "docs_words = {}\n",
    "i = 0 #for docs_words\n",
    "\n",
    "f = open(\"stoplist.txt\", \"r\") \n",
    "stoplist = f.read().splitlines() #Stoplist words\n",
    "\n",
    "ps = PorterStemmer() #stemmer\n",
    "\n",
    "for f in files:\n",
    "    doc = open(f,\"r\")\n",
    "    html = doc.read()\n",
    "    \n",
    "    index = html.find(\"<html\")\n",
    "    if index == -1 :\n",
    "        html.find(\"<Html\")\n",
    "    elif index == -1 : \n",
    "        html.find(\"<HTML\")\n",
    "    html = html[index:]\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    #print(\"Doc : \" + text + \"\\n\\n\")\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(text)\n",
    "    words_pos = {}\n",
    "    j = 0; #for words_pos\n",
    "    for t in tokens:\n",
    "        t = t.lower() #lowering Case\n",
    "        t = ps.stem(t) #stemming\n",
    "        if t not in stoplist and len(t) > 1: #Removing stop words\n",
    "            if t not in words_pos.keys():\n",
    "                words_pos[t] = []\n",
    "            if t in words_pos.keys():\n",
    "                words_pos[t].append(j)\n",
    "            j = j + 1\n",
    "    \n",
    "    docs_words[i] = words_pos\n",
    "    i = i + 1\n",
    "\n",
    "print(docs_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
